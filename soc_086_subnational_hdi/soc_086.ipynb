{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from shapely.geometry import Polygon, Point, mapping\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import dotenv\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import dotenv\n",
    "#insert the location of your .env file here:\n",
    "dotenv.load_dotenv('/home/eduardo/Documents/RW_github/cred/.env')\n",
    "utils_path = os.path.join(os.path.abspath(os.getenv('PROCESSING_DIR')),'utils')\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)\n",
    "import util_files\n",
    "import util_cloud\n",
    "import util_carto\n",
    "import shutil\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing script for dataset: soc_086_subnational_hdi\n"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "# Get the top-level logger object\n",
    "logger = logging.getLogger()\n",
    "for handler in logger.handlers: logger.removeHandler(handler)\n",
    "logger.setLevel(logging.INFO)\n",
    "# make it print to the console.\n",
    "console = logging.StreamHandler()\n",
    "logger.addHandler(console)\n",
    "logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# name of table on Carto where you want to upload data\n",
    "# using preexisting table for this dataset\n",
    "dataset_name = 'soc_086_subnational_hdi' #check\n",
    "\n",
    "logger.info('Executing script for dataset: ' + dataset_name)\n",
    "# create a new sub-directory within your specified dir called 'data'\n",
    "# within this directory, create files to store raw and processed data\n",
    "data_dir = util_files.prep_dirs(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading raw data\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Download data and save to your data directory\n",
    "Data can be downloaded from:\n",
    "https://globaldatalab.org/shdi/view/shdi/\n",
    "\n",
    "Shapefile for subnational boundaries:\n",
    "https://globaldatalab.org/shdi/shapefiles/\n",
    "'''\n",
    "\n",
    "# download the data from the source\n",
    "logger.info('Downloading raw data')\n",
    "downloads = []\n",
    "downloads.append(glob.glob(os.path.join(os.path.expanduser(\"~\"), 'Downloads', 'GDL-Sub-national-HDI-data.csv'))[0])\n",
    "downloads.append(glob.glob(os.path.join(os.path.expanduser(\"~\"), 'Downloads', 'GDL Shapefiles V4.zip'))[0])\n",
    "\n",
    "# move this file into your data directory\n",
    "# Create file paths where the excel files will be stored\n",
    "raw_data_file = [os.path.join(data_dir,os.path.basename(download)) for download in downloads]\n",
    "for index, download in enumerate(downloads):\n",
    "    shutil.move(download,raw_data_file[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip source data\n",
    "raw_data_file_unzipped = raw_data_file[1].split('.')[0]\n",
    "zip_ref = ZipFile(raw_data_file[1], 'r')\n",
    "zip_ref.extractall(raw_data_file_unzipped)\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to open EPSG support file gcs.csv.  Try setting the GDAL_DATA environment variable to point to the directory containing EPSG csv files.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Process data\n",
    "'''\n",
    "\n",
    "# load in the polygon shapefile of sub-national human development index boundaries\n",
    "shapefile= glob.glob(os.path.join(raw_data_file_unzipped, '*.shp' ))[0]\n",
    "gdf_shapefile = gpd.read_file(shapefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproject geometries to epsg 4326\n",
    "gdf_shapefile['geometry'] = gdf_shapefile['geometry'].to_crs(epsg=4326)\n",
    "# convert the column names to lowercase\n",
    "gdf_shapefile.columns = [x.lower() for x in gdf_shapefile.columns]\n",
    "# replace all NaN with None\n",
    "gdf_shapefile=gdf_shapefile.where((pd.notnull(gdf_shapefile)), None)\n",
    "# create an index column to use as cartodb_id\n",
    "gdf_shapefile['cartodb_id'] = gdf_shapefile.index\n",
    "# reorder the columns\n",
    "gdf_shapefile = gdf_shapefile[['cartodb_id'] + list(gdf_shapefile)[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cartodb_id</th>\n",
       "      <th>gdlcode</th>\n",
       "      <th>constant</th>\n",
       "      <th>iso_code</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>shdi</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AFGr101</td>\n",
       "      <td>World</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Central (Kabul Wardak Kapisa Logar Parwan Panj...</td>\n",
       "      <td>0.573915</td>\n",
       "      <td>POLYGON ((69.41367 33.91973, 69.40897 33.91796...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AFGr102</td>\n",
       "      <td>World</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Central Highlands (Bamyan Daikundi)</td>\n",
       "      <td>0.484076</td>\n",
       "      <td>POLYGON ((65.32353 33.12133, 65.34240 33.17651...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AFGr103</td>\n",
       "      <td>World</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>East (Nangarhar Kunar Laghman Nooristan)</td>\n",
       "      <td>0.477673</td>\n",
       "      <td>POLYGON ((70.46561 33.92853, 70.45656 33.92963...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AFGr104</td>\n",
       "      <td>World</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>North (Samangan Sar-e-Pul Balkh Jawzjan Faryab)</td>\n",
       "      <td>0.514833</td>\n",
       "      <td>POLYGON ((66.38873 34.93133, 66.36456 34.93081...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AFGr105</td>\n",
       "      <td>World</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>North East (Baghlan Takhar Badakhshan Kunduz)</td>\n",
       "      <td>0.460395</td>\n",
       "      <td>POLYGON ((67.35538 34.88549, 67.35532 34.88847...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cartodb_id  gdlcode constant iso_code      country  \\\n",
       "0           0  AFGr101    World      AFG  Afghanistan   \n",
       "1           1  AFGr102    World      AFG  Afghanistan   \n",
       "2           2  AFGr103    World      AFG  Afghanistan   \n",
       "3           3  AFGr104    World      AFG  Afghanistan   \n",
       "4           4  AFGr105    World      AFG  Afghanistan   \n",
       "\n",
       "                                              region      shdi  \\\n",
       "0  Central (Kabul Wardak Kapisa Logar Parwan Panj...  0.573915   \n",
       "1                Central Highlands (Bamyan Daikundi)  0.484076   \n",
       "2           East (Nangarhar Kunar Laghman Nooristan)  0.477673   \n",
       "3    North (Samangan Sar-e-Pul Balkh Jawzjan Faryab)  0.514833   \n",
       "4      North East (Baghlan Takhar Badakhshan Kunduz)  0.460395   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((69.41367 33.91973, 69.40897 33.91796...  \n",
       "1  POLYGON ((65.32353 33.12133, 65.34240 33.17651...  \n",
       "2  POLYGON ((70.46561 33.92853, 70.45656 33.92963...  \n",
       "3  POLYGON ((66.38873 34.93133, 66.36456 34.93081...  \n",
       "4  POLYGON ((67.35538 34.88549, 67.35532 34.88847...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf_shapefile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a path to save the processed shapefile later\n",
    "processed_shp_file = os.path.join(data_dir, 'soc_086_subnational_hdi_shapefile_edit'+'.shp')\n",
    "# save processed dataset to shapefile\n",
    "gdf_shapefile.to_file(processed_shp_file,driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Process shdi data from csv file\n",
    "'''\n",
    "\n",
    "# read in data to pandas dataframe\n",
    "df = pd.read_csv(raw_data_file[0])\n",
    "# convert the column names to lowercase\n",
    "df.columns = [x.lower() for x in df.columns]\n",
    "# convert tables from wide form (each year is a column) to long form\n",
    "df = pd.melt(df,id_vars=['country', 'iso_code','level','gdlcode','region'],var_name='year', value_name='shdi_value')\n",
    "# change type of columns\n",
    "df['year'] = df['year'].astype('int64')\n",
    "# create a new column 'datetime' to store years as datetime objects\n",
    "df['datetime'] = [datetime(x, 1, 1) for x in df.year]\n",
    "# save processed dataset to csv\n",
    "processed_data_file = os.path.join(data_dir, dataset_name +'_edit.csv')\n",
    "df.to_csv(processed_data_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Upload original data and processed data to Amazon S3 storage\n",
    "'''\n",
    "\n",
    "# initialize AWS variables\n",
    "aws_bucket = 'wri-public-data'\n",
    "s3_prefix = 'resourcewatch/'\n",
    "\n",
    "logger.info('Uploading original data to S3.')\n",
    "# Upload raw data file to S3\n",
    "\n",
    "# Copy the raw data into a zipped file to upload to S3\n",
    "raw_data_dir = os.path.join(data_dir, dataset_name+'.zip')\n",
    "with ZipFile(raw_data_dir,'w') as zipped:\n",
    "    for file in raw_data_file:\n",
    "        zipped.write(file, os.path.basename(file))\n",
    "\n",
    "# Upload raw data file to S3\n",
    "uploaded = util_cloud.aws_upload(raw_data_dir, aws_bucket, s3_prefix + os.path.basename(raw_data_dir))\n",
    "        \n",
    "# Copy the processed shdi shapefile into a zipped file to upload to S3\n",
    "processed_shp_dir = os.path.join(data_dir, 'soc_086_subnational_hdi_shapefile_edit'+'.zip')\n",
    "# Find al the necessary components of the shapefile \n",
    "processed_shp_files = glob.glob(os.path.join(data_dir, 'soc_086_subnational_hdi_shapefile_edit'+'.*'))\n",
    "with ZipFile(processed_shp_dir,'w') as zip:\n",
    "    for file in processed_shp_files:\n",
    "           zip.write(file, os.path.basename(file))\n",
    "# Upload processed shapefile to S3\n",
    "uploaded = util_cloud.aws_upload(processed_shp_dir, aws_bucket, s3_prefix+os.path.basename(processed_shp_dir))\n",
    "\n",
    "# Copy processed csv into a zipped file to upload to S3\n",
    "processed_data_dir = os.path.join(data_dir, dataset_name+'_edit.zip')\n",
    "with ZipFile(processed_data_dir,'w') as zip:\n",
    "    zip.write(processed_data_file, os.path.basename(processed_data_file))\n",
    "# Upload processed data file to S3\n",
    "uploaded = util_cloud.aws_upload(processed_data_dir, aws_bucket, s3_prefix+os.path.basename(processed_data_dir))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eduardo/anaconda3/envs/po_env/lib/python3.6/site-packages/carto/resources.py:90: FutureWarning: This is part of a non-public CARTO API and may change in the future. Take this into account if you are using this in a production environment\n",
      "  warnings.warn('This is part of a non-public CARTO API and may change in the future. Take this into account if you are using this in a production environment', FutureWarning)\n",
      "Carto table created: soc_086_subnational_hdi_edit\n",
      "Adding the following tags to table: ['rw']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Upload processed data to Carto\n",
    "'''\n",
    "# Upload processed shdi shapefile to carto\n",
    "#util_carto.upload_to_carto(processed_shp_dir, 'LINK')\n",
    "# Upload processed shdi data to carto\n",
    "util_carto.upload_to_carto(processed_data_file, 'LINK')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/GDL-Sub-national-HDI-data.csv', 'data/GDL Shapefiles V4.zip']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.11 64-bit ('po_env': conda)",
   "language": "python",
   "name": "python361164bitpoenvconda60914cbbd79c496e8af95438d56b10d8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
