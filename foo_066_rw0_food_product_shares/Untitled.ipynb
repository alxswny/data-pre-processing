{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import dotenv\n",
    "#insert the location of your .env file here:\n",
    "dotenv.load_dotenv('/home/hastur_2021/documents/rw_github/cred/.env')\n",
    "import sys\n",
    "utils_path = os.path.join(os.path.abspath(os.getenv('PROCESSING_DIR')),'utils')\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)\n",
    "import util_files\n",
    "import util_cloud\n",
    "import util_carto\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing script for dataset: foo_066_rw0_food_product_shares\n"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "# Get the top-level logger object\n",
    "logger = logging.getLogger()\n",
    "for handler in logger.handlers: logger.removeHandler(handler)\n",
    "logger.setLevel(logging.INFO)\n",
    "# make it print to the console.\n",
    "console = logging.StreamHandler()\n",
    "logger.addHandler(console)\n",
    "logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# name of folder script folder\n",
    "# This dataset will be divided in two\n",
    "dataset_name = 'foo_066_rw0_food_product_shares' #check\n",
    "\n",
    "logger.info('Executing script for dataset: ' + dataset_name)\n",
    "# create a new sub-directory within your specified dir called 'data'\n",
    "# within this directory, create files to store raw and processed data\n",
    "data_dir = util_files.prep_dirs(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDownload data and save to your data directory\\nThis script process two datasets related to import and export product shares\\n\\nData for imports can be downloaded with the following link:\\nhttps://wits.worldbank.org/CountryProfile/en/Country/WLD/StartYear/1988/EndYear/2019/TradeFlow/Export/Indicator/XPRT-PRDCT-SHR/Partner/ALL/Product/16-24_FoodProd\\nData for exports can be downloaded with the following link:\\nhttps://wits.worldbank.org/CountryProfile/en/Country/WLD/StartYear/1988/EndYear/2019/TradeFlow/Import/Indicator/MPRT-PRDCT-SHR/Partner/ALL/Product/16-24_FoodProd#\\n\\nTwo excel files was downloaded from the explorer\\nafter selecting the following options from menu:\\n    \\n    Countriy/region: World\\n    Year: 1988-2019\\n    Trade flow: Import/Export\\n    Indicators: Import Product share(%)/Export Product share(%)\\n    View By: Product\\n    Product: Food Products\\n    Partner: By country and region\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Download data and save to your data directory\n",
    "This script process two files related to import and export product shares\n",
    "\n",
    "Data for imports can be downloaded with the following link:\n",
    "https://wits.worldbank.org/CountryProfile/en/Country/WLD/StartYear/1988/EndYear/2019/TradeFlow/Export/Indicator/XPRT-PRDCT-SHR/Partner/ALL/Product/16-24_FoodProd\n",
    "Data for exports can be downloaded with the following link:\n",
    "https://wits.worldbank.org/CountryProfile/en/Country/WLD/StartYear/1988/EndYear/2019/TradeFlow/Import/Indicator/MPRT-PRDCT-SHR/Partner/ALL/Product/16-24_FoodProd#\n",
    "\n",
    "Two excel files was downloaded from the explorer\n",
    "after selecting the following options from menu:\n",
    "    \n",
    "    Countriy/region: World\n",
    "    Year: 1988-2019\n",
    "    Trade flow: Import/Export\n",
    "    Indicators: Import Product share(%)/Export Product share(%)\n",
    "    View By: Product\n",
    "    Product: Food Products\n",
    "    Partner: By country and region\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading raw data\n"
     ]
    }
   ],
   "source": [
    "# download the data from the source\n",
    "logger.info('Downloading raw data')\n",
    "downloads = []\n",
    "downloads.append(glob.glob(os.path.join(os.path.expanduser(\"~\"), 'Downloads', 'WITS-Partner-import.xlsx'))[0])\n",
    "downloads.append(glob.glob(os.path.join(os.path.expanduser(\"~\"), 'Downloads', 'WITS-Partner-export.xlsx'))[0])\n",
    "# Create file paths where the excel files will be stored\n",
    "raw_data_file = [os.path.join(data_dir,os.path.basename(download)) for download in downloads]\n",
    "# We move the files stored in the downloads list\n",
    "for index, element in enumerate(downloads):\n",
    "    shutil.move(element, raw_data_file[index])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Process data\n",
    "'''\n",
    "# We read the product import share dataset first\n",
    "# Go through data directory and append xlsx files into a list\n",
    "# Then concatenate the content of the list in a dataframe\n",
    "\n",
    "df_list = []\n",
    "for file in raw_data_file:\n",
    "    df = pd.read_excel(file, sheet_name = 'Product-TimeSeries-Partner')\n",
    "    df_list.append(df)\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# convert tables from wide form (each year is a column) to long form\n",
    "df_edit = pd.melt(df,id_vars=['Reporter Name','Partner Name','Trade Flow','Product Group',\n",
    "                              'Indicator'],var_name='year', value_name='share_percentage')\n",
    " \n",
    "\n",
    "# replace spaces and special characters in column headers with '_\" \n",
    "df_edit.columns = df_edit.columns.str.replace(' ', '_')\n",
    "df_edit.columns = df_edit.columns.str.replace('/', '_')\n",
    "df_edit.columns = df_edit.columns.str.replace('-', '_')\n",
    "\n",
    "# replace all NaN with None\n",
    "df_edit = df_edit.where((pd.notnull(df_edit)), None)\n",
    "\n",
    "# convert the column names to lowercase\n",
    "df_edit.columns = [x.lower() for x in df_edit.columns]\n",
    "\n",
    "# convert the data type of the column 'year' to integer\n",
    "df_edit['year'] = df_edit['year'].astype('int64')\n",
    "\n",
    "# convert the years in the 'year' column to datetime objects and store them in a new column 'datetime'\n",
    "df_edit['datetime'] = [datetime.datetime(x, 1, 1) for x in df_edit.year]\n",
    "\n",
    "# convert share percentage to float type\n",
    "df_edit['share_percentage'] = df_edit['share_percentage'].astype('float64')\n",
    "\n",
    "# save processed dataset to csv\n",
    "processed_data_file = os.path.join(data_dir, dataset_name+'_edit.csv')\n",
    "df_edit.to_csv(processed_data_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reporter_name               object\n",
       "partner_name                object\n",
       "trade_flow                  object\n",
       "product_group               object\n",
       "indicator                   object\n",
       "year                         int64\n",
       "share_percentage           float64\n",
       "datetime            datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Upload processed data to Carto\n",
    "'''\n",
    "logger.info('Uploading processed data to Carto.')\n",
    "util_carto.upload_to_carto(processed_data_file, 'LINK')\n",
    "'''\n",
    "Upload original data and processed data to Amazon S3 storage\n",
    "'''\n",
    "# initialize AWS variables\n",
    "aws_bucket = 'wri-public-data'\n",
    "s3_prefix = 'resourcewatch/'\n",
    "\n",
    "logger.info('Uploading original data to S3.')\n",
    "# Copy the raw data into a zipped file to upload to S3\n",
    "raw_data_dir = os.path.join(data_dir, dataset_name+'.zip')\n",
    "with ZipFile(raw_data_dir,'w') as zipped:\n",
    "    for file in raw_data_file:\n",
    "        zipped.write(file, os.path.basename(file))\n",
    "# Upload raw data file to S3\n",
    "uploaded = util_cloud.aws_upload(raw_data_dir, aws_bucket, s3_prefix + os.path.basename(raw_data_dir))\n",
    "logger.info('Uploading processed data to S3.')\n",
    "\n",
    "# Copy the processed data into a zipped file to upload to S3\n",
    "processed_data_dir = os.path.join(data_dir, dataset_name+'_edit.zip')\n",
    "with ZipFile(processed_data_dir,'w') as zip:\n",
    "    zip.write(processed_data_file, os.path.basename(processed_data_file)) \n",
    "        \n",
    "# Upload processed data file to S3\n",
    "uploaded = util_cloud.aws_upload(processed_data_dir, aws_bucket, s3_prefix + os.path.basename(processed_data_dir))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
